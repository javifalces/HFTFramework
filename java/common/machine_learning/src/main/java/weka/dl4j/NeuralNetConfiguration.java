/*
 * WekaDeeplearning4j is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * WekaDeeplearning4j is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with WekaDeeplearning4j.  If not, see <https://www.gnu.org/licenses/>.
 *
 * NeuralNetConfiguration.java
 * Copyright (C) 2017-2018 University of Waikato, Hamilton, New Zealand
 */

package weka.dl4j;

import java.io.Serializable;
import java.util.Enumeration;

import lombok.EqualsAndHashCode;
import lombok.ToString;
import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration.Builder;
import org.deeplearning4j.nn.conf.WorkspaceMode;
import org.deeplearning4j.nn.weights.WeightInit;
import weka.core.Option;
import weka.core.OptionHandler;
import weka.core.OptionMetadata;
import weka.dl4j.distribution.Disabled;
import weka.dl4j.distribution.Distribution;
import weka.dl4j.dropout.AbstractDropout;
import weka.dl4j.enums.GradientNormalization;
import weka.dl4j.updater.Adam;
import weka.dl4j.updater.Sgd;
import weka.dl4j.updater.Updater;
import weka.dl4j.weightnoise.AbstractWeightNoise;
import weka.gui.ProgrammaticProperty;

/**
 * A version of DeepLearning4j's NeuralNetConfiguration that implements WEKA option handling.
 *
 * <p>The duplicate code of configuration parameters is necessary since the dl4j
 * NeuralNetConfiguration.Builder object is not serializable which is necessary for the weka GUI.
 *
 * @author Eibe Frank
 * @author Steven Lang
 */
@EqualsAndHashCode @ToString public class NeuralNetConfiguration implements Serializable, OptionHandler {

	private static final long serialVersionUID = -4384295102884151216L;

	protected WeightInit weightInit = WeightInit.XAVIER;
	protected double biasInit = 0.0;
	protected Distribution dist = new Disabled();
	protected double l1 = Double.NaN;
	protected double l2 = Double.NaN;
	protected AbstractDropout dropout = new weka.dl4j.dropout.Disabled();
	protected Updater updater = new Sgd();
	protected Updater biasUpdater = new Sgd();
	protected boolean miniBatch = true;
	protected long seed = 0;
	protected OptimizationAlgorithm optimizationAlgo = OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT;
	protected boolean useDropConnect = false;
	protected AbstractWeightNoise weightNoise = new weka.dl4j.weightnoise.Disabled();
	protected boolean minimize = true;
	protected GradientNormalization gradientNormalization = GradientNormalization.None;
	protected double gradientNormalizationThreshold = 1.0;
	protected WorkspaceMode inferenceWorkspaceMode = Preferences.WORKSPACE_MODE;
	protected WorkspaceMode trainingWorkspaceMode = Preferences.WORKSPACE_MODE;

	/**
	 * Constructor that provides default values for the settings.
	 */
	public NeuralNetConfiguration() {
		setUpdater(new Adam());
		setWeightInit(WeightInit.XAVIER);
	}

	/**
	 * Deliver access to the internal builder
	 *
	 * @return NeuralNetworkConfiguration
	 */
	public Builder builder() {
		Builder builder = new Builder();

		// Set dist to null if Disabled was chosen as dl4j backend defaults to null

		builder.l1(l1).l2(l2).optimizationAlgo(optimizationAlgo).seed(seed).biasInit(biasInit)
				.updater(updater.getBackend()).biasUpdater(biasUpdater.getBackend()).dropOut(dropout.getBackend())
				.miniBatch(miniBatch).minimize(minimize).weightNoise(weightNoise.getBackend())
				.gradientNormalization(gradientNormalization.getBackend())
				.gradientNormalizationThreshold(gradientNormalizationThreshold)
				.inferenceWorkspaceMode(inferenceWorkspaceMode).trainingWorkspaceMode(trainingWorkspaceMode);

		if (!(dist instanceof Disabled)) {
			builder.weightInit(dist.getBackend());
		} else {
			builder.weightInit(weightInit);
		}

		return builder;
	}

	@OptionMetadata(description = "Optimization algorithm (LINE_GRADIENT_DESCENT,"
			+ " CONJUGATE_GRADIENT, HESSIAN_FREE, "
			+ "LBFGS, STOCHASTIC_GRADIENT_DESCENT)", displayName = "optimization algorithm", commandLineParamName = "algorithm", commandLineParamSynopsis = "-algorithm <string>", displayOrder = 1) public OptimizationAlgorithm getOptimizationAlgo() {
		return optimizationAlgo;
	}

	public void setOptimizationAlgo(OptimizationAlgorithm optimAlgorithm) {
		optimizationAlgo = optimAlgorithm;
	}

	@OptionMetadata(displayName = "whether to minimize objective", description = "Whether to minimize objective.", commandLineParamIsFlag = true, commandLineParamName = "minimize", commandLineParamSynopsis = "-minimize", displayOrder = 7) public boolean isMinimize() {
		return minimize;
	}

	public void setMinimize(boolean b) {
		minimize = b;
	}

	@OptionMetadata(displayName = "updater", description = "The updater to use (default = SGD).", commandLineParamName = "updater", commandLineParamSynopsis = "-updater <string>", displayOrder = 12) public Updater getUpdater() {
		return updater;
	}

	public void setUpdater(Updater updater) {
		this.updater = updater;
	}

	@OptionMetadata(displayName = "biasUpdater", description = "The updater to use for the bias (default = SGD).", commandLineParamName = "biasUpdater", commandLineParamSynopsis = "-biasUpdater <string>", displayOrder = 13) public Updater getBiasUpdater() {
		return biasUpdater;
	}

	public void setBiasUpdater(Updater biasUpdater) {
		this.biasUpdater = biasUpdater;
	}

	@OptionMetadata(displayName = "dropout", description = "The dropout method to use (default = Dropout(0.0).", commandLineParamName = "dropout", commandLineParamSynopsis = "-dropout <Dropout>", displayOrder = 25) public AbstractDropout getDropout() {
		return dropout;
	}

	public void setDropout(AbstractDropout dropout) {
		this.dropout = dropout;
	}

	@OptionMetadata(displayName = "weightNoise", description = "The weight noise method to use (default = None).", commandLineParamName = "weightNoise", commandLineParamSynopsis = "-weightNoise <WeightNoise>", displayOrder = 26) public AbstractWeightNoise getWeightNoise() {
		return weightNoise;
	}

	public void setWeightNoise(AbstractWeightNoise weightNoise) {
		this.weightNoise = weightNoise;
	}

	@OptionMetadata(displayName = "l1 regularization factor", description = "L1 regularization factor (default = 0.00).", commandLineParamName = "l1", commandLineParamSynopsis = "-l1 <double>", displayOrder = 14) public double getL1() {
		return l1;
	}

	public void setL1(double l1) {
		this.l1 = l1;
	}

	@OptionMetadata(displayName = "l2 regularization factor", description = "L2 regularization factor (default = 0.00).", commandLineParamName = "l2", commandLineParamSynopsis = "-l2 <double>", displayOrder = 15) public double getL2() {
		return l2;
	}

	public void setL2(double l2) {
		this.l2 = l2;
	}

	@OptionMetadata(displayName = "weight initialization method", description = "The method for weight initialization (default = XAVIER).", commandLineParamName = "weightInit", commandLineParamSynopsis = "-weightInit <specification>", displayOrder = 18) public WeightInit getWeightInit() {
		return weightInit;
	}

	public void setWeightInit(WeightInit weightInit) {
		this.weightInit = weightInit;
	}

	@OptionMetadata(displayName = "gradient normalization method", description = "The gradient normalization method (default = None).", commandLineParamName = "gradientNormalization", commandLineParamSynopsis = "-gradientNormalization <specification>", displayOrder = 22) public GradientNormalization getGradientNormalization() {
		return this.gradientNormalization;
	}

	public void setGradientNormalization(GradientNormalization gradientNormalization) {
		this.gradientNormalization = gradientNormalization;
	}

	@OptionMetadata(displayName = "gradient normalization threshold", description = "The gradient normalization threshold (default = 1).", commandLineParamName = "gradNormThreshold", commandLineParamSynopsis = "-gradNormThreshold <double>", displayOrder = 23) public double getGradientNormalizationThreshold() {
		return this.gradientNormalizationThreshold;
	}

	public void setGradientNormalizationThreshold(double gradientNormalizationThreshold) {
		this.gradientNormalizationThreshold = gradientNormalizationThreshold;
	}

	@OptionMetadata(displayName = "distribution", description = "The weight init distribution type. Only applies when weightinit=DISTRIBUTION (default = Disabled).", commandLineParamName = "dist", commandLineParamSynopsis = "-dist <specification>", displayOrder = 19) public Distribution<? extends org.deeplearning4j.nn.conf.distribution.Distribution> getDist() {
		return dist;
	}

	public void setDist(Distribution<? extends org.deeplearning4j.nn.conf.distribution.Distribution> dist) {
		this.dist = dist;
	}

	@OptionMetadata(displayName = "bias initialization", description = "The bias initialization (default = 0.0).", commandLineParamName = "biasInit", commandLineParamSynopsis = "-biasInit <double>", displayOrder = 20) public double getBiasInit() {
		return this.biasInit;
	}

	public void setBiasInit(double biasInit) {
		this.biasInit = biasInit;
	}

	@ProgrammaticProperty public long getSeed() {
		return seed;
	}

	public void setSeed(long n) {
		seed = n;
	}

	@ProgrammaticProperty public boolean isMiniBatch() {
		return miniBatch;
	}

	public void setMiniBatch(boolean b) {
		miniBatch = b;
	}

	/**
	 * Returns an enumeration describing the available options.
	 *
	 * @return an enumeration of all the available options.
	 */
	public Enumeration<Option> listOptions() {

		return Option.listOptionsForClass(this.getClass()).elements();
	}

	/**
	 * Gets the current settings of the Classifier.
	 *
	 * @return an array of strings suitable for passing to setOptions
	 */
	public String[] getOptions() {

		return Option.getOptions(this, this.getClass());
	}

	/**
	 * Parses a given list of options.
	 *
	 * @param options the list of options as an array of strings
	 * @throws Exception if an option is not supported
	 */
	public void setOptions(String[] options) throws Exception {

		Option.setOptions(options, this, this.getClass());
	}

	/**
	 * Returns a string describing this search method
	 *
	 * @return a description of the search method suitable for displaying in the explorer/experimenter
	 * gui
	 */
	public String globalInfo() {
		return "Class for fine tuning configurations of the network.\n" + "Parameters set as NaN are unused.\n";
	}
}
